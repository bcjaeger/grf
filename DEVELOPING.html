<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Developing • grf</title>

<!-- favicons -->
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="apple-touch-icon.png" />
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="apple-touch-icon-60x60.png" />

<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="bootstrap-toc.css">
<script src="bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script>



  
  <script src="extra.js"></script>

<meta property="og:title" content="Developing" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-title-body">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">grf</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">2.3.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="articles/grf.html">Get started</a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="articles/grf_guide.html">A grf guided tour</a>
    </li>
    <li>
      <a href="articles/rate.html">Assessing heterogeneity with RATE</a>
    </li>
    <li>
      <a href="articles/categorical_inputs.html">Categorical covariates</a>
    </li>
    <li>
      <a href="articles/survival.html">Causal forest with time-to-event data</a>
    </li>
    <li>
      <a href="articles/ate_transport.html">Estimating ATEs on a new target population</a>
    </li>
    <li>
      <a href="articles/muhats.html">Estimating conditional means</a>
    </li>
    <li>
      <a href="articles/diagnostics.html">Evaluating a causal forest fit</a>
    </li>
    <li>
      <a href="articles/llf.html">Local linear forests</a>
    </li>
    <li>
      <a href="articles/policy_learning.html">Policy learning via optimal decision trees</a>
    </li>
    <li>
      <a href="articles/maq.html">Qini curves: Automatic cost-benefit analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="REFERENCE.html">Algorithm reference</a>
</li>
<li>
  <a href="DEVELOPING.html">Developing</a>
</li>
<li>
  <a href="CHANGELOG.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/grf-labs/grf">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="contents col-md-9">
    <div class="page-header">
      <h1>Developing</h1>
    </div>

<div id="developing" class="section level1">

<p><img src="https://raw.githubusercontent.com/grf-labs/grf/master/images/logo/grf_logo_wbg_cropped.png" align="right" height="120"></p>
<p>In addition to providing out-of-the-box forests for quantile regression and instrumental variables, grf provides a framework for creating forests tailored to new statistical tasks. Certain components around splitting and prediction can be swapped out, within the general infrastructure for growing and predicting on trees.</p>
<div id="table-of-contents" class="section level2">
<h2 class="hasAnchor">
<a href="#table-of-contents" class="anchor"></a>Table of Contents</h2>
<ul>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#working-with-the-code">Working with the code</a></li>
<li>
<a href="#r-package">R package</a>
<ul>
<li><a href="#note-for-windows-users">Note for Windows users</a></li>
</ul>
</li>
<li>
<a href="#core-c">Core C++</a>
<ul>
<li><a href="#code-structure">Code structure</a></li>
<li><a href="#creating-a-custom-forest">Creating a custom forest</a></li>
<li><a href="#current-forests-and-main-components">Current forests and main components</a></li>
<li><a href="#tree-splitting-algorithm">Tree splitting algorithm</a></li>
<li><a href="#computing-point-predictions">Computing point predictions</a></li>
</ul>
</li>
</ul>
</div>
<div id="contributing" class="section level2">
<h2 class="hasAnchor">
<a href="#contributing" class="anchor"></a>Contributing</h2>
<p>This repository follows the standard open source protocol and setup with git where there is an abundance of existing resources to get up to speed (see for example the contributing guidelines for well known packages in other languages, like Scikit-learn, Scipy, and pandas)</p>
<p>Condensed greatly, the workflow is to fork this repository, check out a branch, commit your changes (forming an ideally legible commit history), then submitting a pull request explaining your contribution, ideally referring to the issue you created, or the issue you chose to work on.</p>
</div>
<div id="working-with-the-code" class="section level2">
<h2 class="hasAnchor">
<a href="#working-with-the-code" class="anchor"></a>Working with the code</h2>
<p>The core forest implementation is written in C++, with an R interface powered by Rcpp. We recommend using a full-powered C++ IDE such as CLion, Xcode, or Visual Studio when working with the core code.</p>
</div>
<div id="r-package" class="section level2">
<h2 class="hasAnchor">
<a href="#r-package" class="anchor"></a>R package</h2>
<p>To build the R package from source, cd into <code>r-package</code> and run <code>build_package.R</code>. Required development dependencies are listed there. This mimics the tests run when submitting a pull request. Additional online package documentation is built using continuous integration with <a href="https://pkgdown.r-lib.org/">pkgdown</a>. Usage examples in the form of R Markdown files under <em>grf\vignettes</em> are built and rendered and the R method reference (along with the package articles) is displayed according to the layout defined in <em>pkgdown.yml</em>. To build the site locally run <code><a href="https://pkgdown.r-lib.org/reference/build_site.html">pkgdown::build_site()</a></code> from the R package directory.</p>
<p>An alternative development workflow is to use the accompanying grf.Rproj and build and test the package with RStudio’s build menu, which can be convenient for quickly iterating C++/R code changes. Compiling the package locally with <code>PKG_CPPFLAGS="-UNDEBUG"</code> (or set in <code>~/.R/Makevars</code>) may give helpful debug assertions, as the Eigen library will then perform bounds checks on matrix algebra.</p>
<div id="note-for-windows-users" class="section level3">
<h3 class="hasAnchor">
<a href="#note-for-windows-users" class="anchor"></a>Note for Windows users:</h3>
<p>Symlinks in the src directory point to the core C++ and R bindings. On Windows one has to clone this repository with symlinks enabled: <code>git clone -c core.symlinks=true https://github.com/grf-labs/grf.git</code> (this command may have to be run as an administrator if the account does not have permission to create symlinks). Caveat: the above RStudio workflow is not tested on Windows.</p>
</div>
</div>
<div id="core-c" class="section level2">
<h2 class="hasAnchor">
<a href="#core-c" class="anchor"></a>Core C++</h2>
<div id="code-structure" class="section level3">
<h3 class="hasAnchor">
<a href="#code-structure" class="anchor"></a>Code structure</h3>
<p><img src="https://raw.githubusercontent.com/grf-labs/grf/master/images/arch_diagram.png" alt="GRF Architecture Diagram"></p>
<p>The forest implementation is composed of two top-level components, <a href="https://github.com/grf-labs/grf/blob/master/core/src/forest/ForestTrainer.h">ForestTrainer</a> and <a href="https://github.com/grf-labs/grf/blob/master/core/src/forest/ForestPredictor.h">ForestPredictor</a>.</p>
<p>ForestTrainer drives the tree-growing process, and has two pluggable components.</p>
<ul>
<li>
<a href="https://github.com/grf-labs/grf/blob/master/core/src/relabeling/RelabelingStrategy.h">RelabelingStrategy</a> is applied before every split, and produces a set of relabelled outcomes given the observations for a group of samples. In the case of quantile forests, for example, this strategy computes the quantiles for the group of samples, then relabels them with a factor corresponding to the quantile they belong to.</li>
<li>
<a href="https://github.com/grf-labs/grf/blob/master/core/src/splitting/SplittingRule.h">SplittingRule</a> is called to find the best split for a particular node, given a set of outcomes. There are currently implementations for standard regression and multinomial splitting.</li>
</ul>
<p>The trained forest produces a <a href="https://github.com/grf-labs/grf/blob/master/core/src/forest/Forest.h">Forest</a> object. This can then be passed to the ForestPredictor to predict on test samples. The predictor has a pluggable ‘prediction strategy’, which computes a prediction given a test sample. Prediction strategies can be one of two types:</p>
<ul>
<li>
<a href="https://github.com/grf-labs/grf/blob/master/core/src/prediction/DefaultPredictionStrategy.h">DefaultPredictionStrategy</a> computes a prediction given a weighted list of training sample IDs that share a leaf with the test sample. Taking quantile forests as an example, this strategy would compute the quantiles of the weighted leaf samples.</li>
<li>
<a href="https://github.com/grf-labs/grf/blob/master/core/src/prediction/OptimizedPredictionStrategy.h">OptimizedPredictionStrategy</a> does not predict using a list of neighboring samples and weights, but instead precomputes summary values for each leaf during training, and uses these during prediction. This type of strategy will also be passed to ForestTrainer, so it can define how the summary values are computed. The section on computing point predictions provides more details.</li>
</ul>
<p>Prediction strategies can also compute variance estimates for the predictions, given a forest trained with grouped trees. Because of performance constraints, only ‘optimized’ prediction strategies can provide variance estimates.</p>
<p>A particular type of forest is created by pulling together a set of pluggable components. As an example, a quantile forest is composed of a QuantileRelabelingStrategy, ProbabilitySplittingRule, and QuantilePredictionStrategy. The factory classes <a href="https://github.com/grf-labs/grf/blob/master/core/src/forest/ForestTrainers.h">ForestTrainers</a> and <a href="https://github.com/grf-labs/grf/blob/master/core/src/forest/ForestPredictors.h">ForestPredictors</a> define the common types of forests like regression, quantile, and causal forests.</p>
</div>
<div id="creating-a-custom-forest" class="section level3">
<h3 class="hasAnchor">
<a href="#creating-a-custom-forest" class="anchor"></a>Creating a custom forest</h3>
<p>When designing a forest for a new statistical task, we suggest following the format suggested in the paper: provide a custom RelabelingStrategy and PredictionStrategy, but use the standard RegressionSplittingRule. If modifications to the splitting rule must be made, they should be done carefully, as it is an extremely performance-sensitive piece of code.</p>
<p>To get started with setting up new classes and Rcpp bindings, we suggest having a look at one of the simpler forests, like regression_forest, and use this as a template.</p>
</div>
<div id="current-forests-and-main-components" class="section level3">
<h3 class="hasAnchor">
<a href="#current-forests-and-main-components" class="anchor"></a>Current forests and main components</h3>
<p>The following table shows the current collection of forests implemented and the C++ components.</p>
<table>
<colgroup>
<col width="27%">
<col width="24%">
<col width="21%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th>R forest name</th>
<th>RelabelingStrategy</th>
<th>SplittingStrategy</th>
<th>PredictionStrategy</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>causal_forest</td>
<td>InstrumentalRelabelingStrategy</td>
<td>InstrumentalSplittingRule</td>
<td>InstrumentalPredictionStrategy</td>
</tr>
<tr class="even">
<td>causal_forest with ll_causal_predict</td>
<td>InstrumentalRelabelingStrategy</td>
<td>InstrumentalSplittingRule</td>
<td>LLCausalPredictionStrategy</td>
</tr>
<tr class="odd">
<td>causal_survival_forest</td>
<td>CausalSurvivalRelabelingStrategy</td>
<td>CausalSurvivalSplittingRule</td>
<td>CausalSurvivalPredictionStrategy</td>
</tr>
<tr class="even">
<td>instrumental_forest</td>
<td>InstrumentalRelabelingStrategy</td>
<td>InstrumentalSplittingRule</td>
<td>InstrumentalPredictionStrategy</td>
</tr>
<tr class="odd">
<td>ll_regression_forest</td>
<td>LLRegressionRelabelingStrategy</td>
<td>RegressionSplittingRule</td>
<td>LocalLinearPredictionStrategy</td>
</tr>
<tr class="even">
<td>lm_forest</td>
<td>MultiCausalRelabelingStrategy</td>
<td>MultiRegressionSplittingRule</td>
<td>MultiCausalPredictionStrategy</td>
</tr>
<tr class="odd">
<td>multi_arm_causal_forest</td>
<td>MultiCausalRelabelingStrategy</td>
<td>MultiCausalSplittingRule</td>
<td>MultiCausalPredictionStrategy</td>
</tr>
<tr class="even">
<td>multi_regression_forest</td>
<td>MultiNoopRelabelingStrategy</td>
<td>MultiRegressionSplittingRule</td>
<td>MultiRegressionPredictionStrategy</td>
</tr>
<tr class="odd">
<td>probability_forest</td>
<td>NoopRelabelingStrategy</td>
<td>ProbabilitySplittingRule</td>
<td>ProbabilityPredictionStrategy</td>
</tr>
<tr class="even">
<td>quantile_forest</td>
<td>QuantileRelabelingStrategy</td>
<td>ProbabilitySplittingRule</td>
<td>QuantilePredictionStrategy</td>
</tr>
<tr class="odd">
<td>regression_forest</td>
<td>NoopRelabelingStrategy</td>
<td>RegressionSplittingRule</td>
<td>RegressionPredictionStrategy</td>
</tr>
<tr class="even">
<td>survival_forest</td>
<td>NoopRelabelingStrategy</td>
<td>SurvivalSplittingRule</td>
<td>SurvivalPredictionStrategy</td>
</tr>
</tbody>
</table>
</div>
<div id="tree-splitting-algorithm" class="section level3">
<h3 class="hasAnchor">
<a href="#tree-splitting-algorithm" class="anchor"></a>Tree splitting algorithm</h3>
<p>The follow section outlines pseudocode for some of the components listed above.</p>
<p>Each splitting rule follow the same pattern: given a set of samples, and a possible split variable, iterate over all the split points for that variable and compute an error criterion on both sides of the split, then select the split where the decrease in the summed error criterion is smallest. For <code>RegressionSplittingRule</code> the criterion is the mean squared error, and is usually referred to as standard CART splitting. <code>InstrumentalSplittingRule</code> use the same error criterion, but it incorporates more constraints on potential splits, such as requiring a certain number of treated and control observations (more details are in the <a href="https://grf-labs.github.io/grf/REFERENCE.html">Algorithm Reference</a>).</p>
<p>Two added features to grf’s splitting rule beyond basic CART is the addition of sample weights and support for missing values with Missingness Incorporated in Attributes (MIA) splitting. The algebra behind the decrease calculation is detailed below.</p>
<p><strong><em>Algorithm</em></strong> (<code>RegressionSplittingRule</code>): find the best split for variable <code>var</code> at n samples <code>samples = i...j</code></p>
<p><em>Input</em>: <code>X</code>: covariate matrix, <code>weights</code>: weight n-vector, <code>response</code>: the responses (pseudo-outcomes) n-vector, <code>min_child_size</code>: split constraint on number of samples</p>
<p><em>Output</em>: The best split value and the direction to send missing values</p>
<pre><code>let x = X[samples, var]

for each unique value u in x:
  let count_left[u] be the number of samples with x &lt;= u
  let sum_left[u], weight_sum_left[u] be the sum of weight * response and weight with x &lt;= u

  let count_right[u], sum_right[u], weight_sum_right[u] be the same for samples with x &gt; u
  let count_missing, sum_missing, weight_sum_missing be the same for samples with x = missing

  decrease[u, missing = on_left] = (sum_left[u] + sum_missing)^2 / (weight_sum_left[u] + weight_sum_missing)
                                  + sum_right[u]^2 / weight_sums_right[u]

  decrease[u, missing = on_right] = sum_left[u]^2 / weight_sum_left[u]
                                  + (sum_right[u] + sum_missing)^2 / (weight_sums_right[u] + weight_sum_missing)

return (u, missing) that maximizes decrease such that
count_left[u] and count_right[u] &gt;= min_child_size</code></pre>
<hr>
<p><strong><em>Algorithm</em></strong> (Decrease calculation in weighted CART): find the best split <code>Sj</code> for variable <code>Xj</code>. The impurity criterion is the sum of weighted mean squared errors, which for a specific split <code>Sj</code> is minimized by the weighted sample average <code>Ybar</code> (see for example <a href="https://hastie.su.domains/ElemStatLearn/">Elements of Statistical Learning, Ch 9</a>).</p>
<p><em>Input</em>: <code>Xj</code> : Covariate j, <code>w</code>: sample weight vector, <code>y</code>: response vector</p>
<p><em>Output</em>: The best split criterion</p>
<pre><code>minimize wrt. s: MSE(s, left) + MSE(s, right)

where

MSE(s, left) = \sum_{i \in L(s)} wi [yi - Ybar_L(s)]^2
MSE[s, right] = \sum_{i \in R(s)} wi [yi - Ybar_R(s)]^2

L(s) are all points Xj such that Xj &lt;= s, and R(s) are all points such that Xj &gt; s.
Ybar_L(s) and Ybar_R(s) are the weighted sample averages for the respective partitions.

Expand the child impurity and write out the averages (minimize wrt. s):
= \sum_{i \in L(s)} wi yi^2 - W_L(s) [Ybar_L(s)]^2
 + \sum_{i \in R(s)} wi yi^2 - W_R(s) [Ybar_R(s)]^2
= \sum_i wi yi^2
  - 1/W_L(s) [\sum_{i \in L(s)} wi yi]^2
  - 1/W_R(s) [\sum_{i \in R(s)} wi yi]^2

Where W is the sum of sample weights.

The parent impurity is (minimize wrt. s):
= \sum_i wi [yi - Ybar]^2
= \sum_i wi yi^2 - W Ybar^2
= \sum_i wi yi^2 - 1/W [\sum_i wi yi]^2

Child impurity &lt;= parent impurity then reduces to

1/W_L(s) [\sum_{i \in L(s)} wi yi]^2 + 1/W_R(s) [\sum_{i \in R(s)} wi yi]^2 &gt;= 1/W [\sum_i wi yi]^2

Or sum_left^2 / weight_sum_left + sum_right^2 / weight_sum_right</code></pre>
<p>For multivariate CART <code>sum_left^2</code> and <code>sum_right^2</code> becomes the squared L2 norm.</p>
<hr>
<p><strong><em>Algorithm</em></strong> (<code>SurvivalSplittingRule</code>): find the best split for variable <code>var</code> at n samples <code>samples = i...j</code>.</p>
<p><em>Input</em>: <code>X</code>: covariate matrix, <code>response</code>: the failure times, <code>min_child_size</code>: split constraint on number of failures, <code>m</code>: the number of failure times in this node.</p>
<p><em>Output</em>: The best split value and the direction to send missing values</p>
<p>grf performs survival splits by finding the partition that maximizes the <a href="https://en.wikipedia.org/wiki/Logrank_test">log-rank</a> test for the comparison of the survival distribution in the left and right node. The statistic is:</p>
<pre><code>logrank(x, split.value) =  sum over all times k up to m
  (dk,l - Yk,l * dk/Yk)  /
  (Yk,l / Yk * (1 - Yk,l / Yk) * (Yk - dk) / (Yk - 1) dk)

dk,l: the count of failures in the left node at time k
Yk,l: the count of observations at risk in the left node at time k (At risk: all i such that Ti &gt;= k)
dk: the count of failures in the parent node at time k
Yk: the count of observations at risk in the parent node at time k</code></pre>
<p>Note that the absolute value of the response variable (the failure time) is never used, only counts matter, constructed from the relative ordering between them. Therefore, the response variable is always relabeled to range from consecutive integers starting at zero and ending at the number of failures. This can be done quickly with binary search:</p>
<pre><code>let failure_times be a sorted vector of the original failure values (T1, T2, .. TM)
initialize response_relabeled = zeros(n)
for each i=1:n
  relabeled_failure_time = binary_search(response[i], failure_times)
  response_relabeled[i] = relabeled_failure_time</code></pre>
<p>A label of 0 is given to all samples with response less than the first failure time, a value of 1 is given to all samples with failure value greater or equal to the first failure value and less than the second failure value, etc.</p>
<p>The algorithm proceeds in the same manner as outlined above for RegressionSplitting: iterate over all possible splits and calculate the logrank statistic, one with all missing values on the left, and one with all missing on the right. Then select the split that yielded the maximum logrank test, subject to a constraint that there are a sufficient amount of failures on both sides of the split.</p>
<hr>
<p><strong><em>Algorithm</em></strong> (<code>ProbabilitySplittingRule</code>): This splitting rule uses the Gini impurity measure from CART for categorical data. Sample weights are incorporated by counting the sample weight of an observation when forming class counts. The missing values adjustment is the same as for the other splitting rules.</p>
</div>
<div id="computing-point-predictions" class="section level3">
<h3 class="hasAnchor">
<a href="#computing-point-predictions" class="anchor"></a>Computing point predictions</h3>
<p>GRF point estimates, <span class="math inline"><em>θ</em>(<em>x</em>)</span>, for a target sample <span class="math inline"><em>X</em> = <em>x</em></span>, are given by a forest-specific estimating equation <span class="math inline"><em>ψ</em><sub><em>θ</em>(<em>x</em>)</sub>( ⋅ )</span> solved using GRF forest weights <span class="math inline"><em>α</em>(<em>x</em>)</span> (see equation (2) and (3) in the <a href="https://arxiv.org/abs/1610.01271">GRF paper</a>). In the GRF software package we additionally incorporate sample weights <span class="math inline"><em>w</em><sub><em>i</em></sub></span>. For training samples <span class="math inline"><em>i</em> = 1...<em>n</em></span>, predictions at a target sample <span class="math inline"><em>X</em> = <em>x</em></span> are given by:</p>
<p><br><span class="math display">$$
\sum_{i = 1}^{n} \alpha_i(x) w_i \psi_{\theta(x)}(\cdot) = 0.
$$</span><br></p>
<p>This is the construction the <code>DefaultPredictionStrategy</code> operates with. The forest weights <span class="math inline"><em>α</em><sub><em>i</em></sub>(<em>x</em>)</span> are available through the method parameter <code>weights_by_sample</code>: entry i is the fraction of times the i-th training sample falls into the same leaf as the target sample <span class="math inline"><em>x</em></span>.</p>
<p>For performance reasons, this is not the construction used for most forest predictions, since for many statistical tasks doing this summation over n training samples times the number of target samples involves redundant (repeated) computations. The <code>OptimizedPredictionStrategy</code> avoids this drawback by relying on precomputed forest-specific ‘sufficient’ statistics.</p>
<p><strong><em>Example: regression forest</em></strong></p>
<p>We’ll illustrate with an example for regression forest. The estimating equation for the conditional mean <span class="math inline"><em>μ</em>(<em>x</em>)</span> is <span class="math inline"><em>ψ</em><sub><em>μ</em>(<em>x</em>)</sub>(<em>Y</em><sub><em>i</em></sub>) = <em>Y</em><sub><em>i</em></sub> − <em>μ</em>(<em>x</em>)</span>. Plugging this into the moment equation above gives:</p>
<p><br><span class="math display">$$
\sum_{i = 1}^{n} \alpha_i(x) w_i (Y_i - \mu(x)) = 0.
$$</span><br></p>
<p>Solving for <span class="math inline"><em>μ</em>(<em>x</em>)</span> gives</p>
<p><br><span class="math display">$$
\mu(x) =  \frac{\sum_{i = 1}^{n} \alpha_i(x) w_i Y_i}{\sum_{i = 1}^{n} \alpha_i(x) w_i}.
$$</span><br></p>
<p>Now, insert the definition of the forest weights:</p>
<p><br><span class="math display">$$
\alpha_i(x) = \frac{1}{B}\sum_{b=1}^{B} \frac{I(X_i\in L_b(x) )}{|L_b(x)|},
$$</span><br></p>
<p>where <span class="math inline"><em>L</em><sub><em>b</em></sub>(<em>x</em>)</span> is the set of training samples falling into the same leaf as <span class="math inline"><em>x</em></span> in the b-th tree and <span class="math inline">|<em>L</em><sub><em>b</em></sub>(<em>x</em>)|</span> is the cardinality of this set. This gives us:</p>
<p><br><span class="math display">$$
\mu(x) = \frac{\sum_{i = 1}^{n} \bigg(\frac{1}{B}\sum_{b=1}^{B} \frac{I(X_i\in L_b(x) )}{|L_b(x)|}\bigg) w_i Y_i}{\sum_{i = 1}^{n} \bigg(\frac{1}{B}\sum_{b=1}^B \frac{I(X_i\in L_b(x) )}{|L_b(x)|}\bigg) w_i}.
$$</span><br></p>
<p>Now we change the order of summation, moving the summation over trees outside:</p>
<p><br><span class="math display">$$
\begin{split}
    \mu(x) &amp;= \frac{\frac{1}{B} \sum_{b = 1}^{B} \bigg( \frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i Y_i\bigg)}{\frac{1}{B} \sum_{b = 1}^{B} \bigg(\frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i\bigg)} \\
    &amp;= \frac{\frac{1}{B} \sum_{b = 1}^{B}  \bar Y_b(x)} {\frac{1}{B} \sum_{b = 1}^{B}  \bar w_b(x)},
\end{split}
$$</span><br></p>
<p>where we define the two tree-specific sufficient statistics <span class="math inline"><em>Ȳ</em><sub><em>b</em></sub>(<em>x</em>)</span> and <span class="math inline"><em>w̄</em><sub><em>b</em></sub>(<em>x</em>)</span> as:</p>
<p><br><span class="math display">$$
\bar Y_b(x) =\frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i Y_i,
$$</span><br></p>
<p>and</p>
<p><br><span class="math display">$$
\bar w_b(x) =\frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i.
$$</span><br></p>
<p>That is, computing a point estimate for <span class="math inline"><em>μ</em>(<em>x</em>)</span> now involves a summation of B pre-computed sufficient statistic (done during training in the method <code>precompute_prediction_values</code>).</p>
<p><strong><em>Example: causal forest</em></strong></p>
<p>Causal forest and instrumental forest shares the same implementation since a causal forest equals an instrumental forest with the treatment assignment vector equal to the instrument. For simplicity, we here just state what the above decomposition would look like for a causal forest (see section 6.1 of the GRF paper), given centered outcomes and treatment assignments <span class="math inline">(<em>Ỹ</em><sub><em>i</em></sub>, <em>W̃</em><sub><em>i</em></sub></span>) (see section 6.1.1 of the GRF paper). The point predictions <span class="math inline"><em>τ</em>(<em>x</em>)</span> for target sample <span class="math inline"><em>X</em> = <em>x</em></span> are given by:</p>
<p><br><span class="math display">$$
\tau(x) =
\frac{\frac{1}{B} \sum_{b = 1}^{B} \bigg(  \bar {YW}_b(x) \bar w_b(x) - \bar Y_b(x) \bar W_b(x)  \bigg)} {\frac{1}{B} \sum_{b = 1}^{B} \bigg(  \bar W^2_b(x) \bar w_b(x) - \bar W_b(x) \bar W_b(x) \bigg) },
$$</span><br></p>
<p>where</p>
<p><br><span class="math display">$$
\bar Y_b(x) = \frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i \tilde Y_i,
$$</span><br></p>
<p><br><span class="math display">$$
\bar W_b(x) = \frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i \tilde W_i,
$$</span><br></p>
<p><br><span class="math display">$$
\bar W^2_b(x) = \frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i \tilde W^2_i,
$$</span><br></p>
<p><br><span class="math display">$$
\bar w_b(x) =\frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i,
$$</span><br></p>
<p><br><span class="math display">$$
\bar {YW}_b(x) =\frac{1}{|L_b(x)|}\sum_{i=1}^{n} I(X_i\in L_b(x) ) w_i \tilde Y_i \tilde W_i.
$$</span><br></p>
</div>
</div>
</div>

  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>

</div>



      <footer>
      <div class="copyright">
  <p>Developed by Julie Tibshirani, Susan Athey, Erik Sverdrup, Stefan Wager.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


