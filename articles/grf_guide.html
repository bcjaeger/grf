<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>A grf guided tour • grf</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><script src="../extra.js"></script><meta property="og:title" content="A grf guided tour">
<meta property="og:description" content="grf">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">grf</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">2.3.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/grf.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/grf_guide.html">A grf guided tour</a>
    </li>
    <li>
      <a href="../articles/rate.html">Assessing heterogeneity with RATE</a>
    </li>
    <li>
      <a href="../articles/categorical_inputs.html">Categorical covariates</a>
    </li>
    <li>
      <a href="../articles/survival.html">Causal forest with time-to-event data</a>
    </li>
    <li>
      <a href="../articles/ate_transport.html">Estimating ATEs on a new target population</a>
    </li>
    <li>
      <a href="../articles/muhats.html">Estimating conditional means</a>
    </li>
    <li>
      <a href="../articles/diagnostics.html">Evaluating a causal forest fit</a>
    </li>
    <li>
      <a href="../articles/llf.html">Local linear forests</a>
    </li>
    <li>
      <a href="../articles/policy_learning.html">Policy learning via optimal decision trees</a>
    </li>
    <li>
      <a href="../articles/maq.html">Qini curves: Automatic cost-benefit analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="../REFERENCE.html">Algorithm reference</a>
</li>
<li>
  <a href="../DEVELOPING.html">Developing</a>
</li>
<li>
  <a href="../CHANGELOG.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/grf-labs/grf">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="grf_guide_files/header-attrs-2.25/header-attrs.js"></script><script src="grf_guide_files/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="grf_guide_files/htmlwidgets-1.6.3/htmlwidgets.js"></script><script src="grf_guide_files/viz-1.8.2/viz.js"></script><link href="grf_guide_files/DiagrammeR-styles-0.2/styles.css" rel="stylesheet">
<script src="grf_guide_files/grViz-binding-1.0.10/grViz.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>A grf guided tour</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/grf-labs/grf/blob/master/vignettes/grf_guide.Rmd"><code>vignettes/grf_guide.Rmd</code></a></small>
      <div class="hidden name"><code>grf_guide.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">grf</span>)</pre></body></html></div>
<center>
<img src="https://raw.githubusercontent.com/grf-labs/grf/master/images/logo/grf_logo_green.png" height="132">
</center>
<p>This vignette gives a <a href="#a-grf-overview">brief overview</a> of the GRF algorithm and contains two example applications: 1) <a href="#application-school-program-evaluation">The benefits of a financial education program</a> and 2) <a href="#application-measuring-the-effect-of-poverty-on-attention">Measuring the effect of poverty on attention</a>, which walks through using GRF to estimate conditional average treatment effects (CATEs) using:</p>
<ul>
<li><p>The <a href="https://grf-labs.github.io/grf/reference/best_linear_projection.html">best_linear_projection</a> as simple linear association measures which can provide a useful lower dimensional summary of the CATEs, that has desirable semi-parametric inferential properties.</p></li>
<li><p>The <a href="https://grf-labs.github.io/grf/reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a> (<em>RATE</em>) as a generic tool to assess heterogeneity and the effectiveness of “targeting rules”, as well as how the associated <em>TOC</em> curve can help identify segments of a population that respond differently to a treatment.</p></li>
<li><p><a href="https://github.com/grf-labs/policytree">policytree</a> to find a tree-based policy using the estimated CATEs.</p></li>
</ul>
<div id="a-grf-overview" class="section level2">
<h2 class="hasAnchor">
<a href="#a-grf-overview" class="anchor"></a>A grf overview</h2>
<p>This section gives a lightning tour of some of the conceptual ideas behind GRF in the form of a walkthrough of how <em>Causal Forest</em> works. It starts with describing how the predictive capabilities of the modern machine learning toolbox can be leveraged to non-parametrically control for confounding when estimating average treatment effects, and how Breiman’s (2001) random forest can be repurposed as an adaptive nearest neighbor finder to detect treatment effect heterogeneity.</p>
<div id="machine-learning-for-causal-inference" class="section level3">
<h3 class="hasAnchor">
<a href="#machine-learning-for-causal-inference" class="anchor"></a>Machine learning for causal inference</h3>
<p>Machine learning algorithms are great at prediction. In the context of estimating treatment effects, however, what matters is estimation and inference (for a great overview of the fundamental tension between prediction and estimation, see the essay by Efron (2020)). The following section outlines how, if carefully adapted and paired with best practices from semi-parametric statistics, modern machine learning can be used to augment the causal inference toolbox with more model-agnostic tools.</p>
<p>Imagine we observe outcomes <span class="math inline">\(Y_i\)</span> along with a binary treatment indicator <span class="math inline">\(W=\{0, 1\}\)</span> and are interested in measuring the average causal effect of the intervention on the outcome. If we are not in a randomized control trial setting but have collected a set of covariates <span class="math inline">\(X_i\)</span> which we believe could plausibly account for confounding, then we could run a regression of the type (Imbens &amp; Rubin, 2005)</p>
<p><span class="math display">\[
Y_i \sim \tau W_i + \beta X_i,
\]</span> and interpret the estimated <span class="math inline">\(\hat \tau\)</span> as an estimate of the average treatment effect <span class="math inline">\(\tau = E[Y_i(1) - Y_i(0)]\)</span>, where <span class="math inline">\(Y_i(1), Y_i(0)\)</span> are potential outcomes corresponding to the two treatment states. This approach is justified if:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(W_i\)</span> is unconfounded given <span class="math inline">\(X_i\)</span> (i.e., treatment is as good as random given covariates).</li>
<li>The confounders <span class="math inline">\(X_i\)</span> have a linear effect on <span class="math inline">\(Y_i\)</span>.</li>
<li>The treatment effect <span class="math inline">\(\tau\)</span> is constant.</li>
</ol>
<p>Assumption 1) is an “identifying” assumption we have to live with, however 2) and 3) are modeling assumptions that we can question. Let’s start with assumption 2): this is a strong parametric modeling assumption that requires the confounders to have a linear effect on the outcome, and that we should be able to relax by relying on semi-parametric statistics.</p>
<p><strong>Relaxing assumption 1)</strong>. We can instead posit the partially linear model:</p>
<p><span class="math display">\[
Y_i = \tau W_i + f(X_i) + \varepsilon_i, \, \, E[\varepsilon_i | X_i, W_i] = 0,
\]</span> where we assume a unit’s baseline outcome is given by some unknown (possibly complex) function <span class="math inline">\(f\)</span>, and that a treatment assignment shifts the outcome by a constant offset <span class="math inline">\(\tau\)</span>. But how do we get around estimating <span class="math inline">\(\tau\)</span> when we do not know <span class="math inline">\(f(X_i)\)</span>? Robinson (1988) shows that, if we define the following two intermediary objects:</p>
<p><span class="math display">\[
e(x) = E[W_i | X_i=x],
\]</span> the propensity score, and</p>
<p><span class="math display">\[
m(x) = E[Y_i | X_i = x] = f(x) + \tau e(x),
\]</span> the conditional mean of <span class="math inline">\(Y\)</span>, then we can rewrite the above equation in “centered” form:</p>
<p><span class="math display">\[
Y_i - m(x) = \tau (W_i - e(x)) + \varepsilon_i.
\]</span> This formulation has great practical appeal, as it means <span class="math inline">\(\tau\)</span> can be estimated by residual-on-residual regression: plug in estimates of <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span> then regress the centered outcomes on the centered treatment indicator. Robinson (1988) shows that this approach yields root-n consistent estimates of <span class="math inline">\(\tau\)</span>, even if estimates of <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span> converge at a slower rate (“4-th root” in particular). This property is often referred to as <em>orthogonality</em> and is a desirable property that essentially tells you that given noisy “nuisance” estimates (<span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span>) you can still recover “good” estimates of your target parameter (<span class="math inline">\(\tau\)</span>).</p>
<p>But how are you supposed to estimate <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span>, don’t you still have to posit a parametric model (like for example a logistic regression for the propensity score)? This is where the modern machine learning toolkit comes in handy, by recognizing that for <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span> we are only asking for good <em>predictions</em> of the outcome conditional on <span class="math inline">\(X_i\)</span>, and the treatment assignment conditional on <span class="math inline">\(X_i\)</span>. This is the type of problem modern machine learning methods like boosting, random forests, etc., excel at. Directly plugging in estimates of <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span> into the residual-on-residual regression will typically lead to bias as modern ML methods regularize to trade of bias and variance, however, Zheng &amp; van der Laan (2011) and Chernozhukov et al. (2018) show that if you form these estimates in a “cross-fit” manner, meaning the prediction of observation i’s outcome and treatment assignment is obtained without using unit i for estimation, then (given some regularity assumptions) you can still recover root-n consistent estimates of <span class="math inline">\(\tau\)</span>.</p>
<p>So, to recap so far: we have a way to adopt the modern ML toolkit to <em>non-parametrically control for confounding</em> when estimating an average treatment effect, and still retain desirable statistical properties such as unbiasedness and consistency.</p>
<p><strong>Non-constant treatment effects</strong>. What if we wish to relax assumption 3)? We could specify certain subgroups and run separate regressions for each subgroup and obtain different estimates of <span class="math inline">\(\tau\)</span>. To avoid false discoveries, this approach would require us to specify potential subgroups without looking at the data. How can we use the data to inform us of potential subgroups? We can continue with Robinson’s partially linear model from above, and instead just posit:</p>
<p><span class="math display">\[
Y_i = \color{red}{\tau(X_i)}W_i + f(X_i) + \varepsilon_i, \, \, E[\varepsilon_i | X_i, W_i] = 0,
\]</span> where <span class="math inline">\(\color{red}{\tau(X_i)}\)</span> is the conditional average treatment effect <span class="math inline">\(E[Y(1) - Y(0) | X_i = x]\)</span>. How do we estimate this? If we imagine we had access to some neighborhood <span class="math inline">\(\mathcal{N}(x)\)</span> where <span class="math inline">\(\tau\)</span> was constant, we could proceed exactly as before, by doing a residual-on-residual regression on the samples belonging to <span class="math inline">\(\mathcal{N}(x)\)</span>, i.e.:</p>
<p><span class="math display">\[
\tau(x) := lm\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i), \text{weights} = 1\{X_i \in \mathcal{N}(x) \}\Biggr),
\]</span> (the superscript <span class="math inline">\(^i\)</span> denotes cross-fit estimates). This is conceptually what <em>Causal Forest</em> does, it estimates the treatment effect <span class="math inline">\(\tau(x)\)</span> for a target sample <span class="math inline">\(X_i = x\)</span> by running a weighted residual-on-residual regression on samples that have similar treatment effects. These <em>weights</em> play a crucial role, so how does <code>grf</code> find them? The next section quickly recaps Breiman’s random forest for regression, then explains how it can be extended to an adaptive neighborhood finder.</p>
</div>
<div id="random-forest-as-an-adaptive-neighborhood-finder" class="section level3">
<h3 class="hasAnchor">
<a href="#random-forest-as-an-adaptive-neighborhood-finder" class="anchor"></a>Random forest as an adaptive neighborhood finder</h3>
<p>Breiman’s random forest for predicting the conditional mean <span class="math inline">\(\mu(x) = E[Y_i | X_i = x]\)</span> can be briefly summarized in two steps:</p>
<ol style="list-style-type: decimal">
<li>Building phase: Build <span class="math inline">\(B\)</span> trees which greedily place covariate splits that maximize the squared difference in subgroups means <span class="math inline">\(n_L n_R (\bar y_L - \bar y_R)^2\)</span>.</li>
<li>Prediction phase: Aggregate each tree’s prediction to form the final point estimate by averaging the outcomes <span class="math inline">\(Y_i\)</span> that fall into the same terminal leaf <span class="math inline">\(L_b(X_i)\)</span> as the targets sample <span class="math inline">\(x\)</span>: <span class="math inline">\(\hat \mu(x) = \frac{1}{B} \sum_{b=1}^{B} \sum_{i=1}^{n} \frac{Y_i 1\{Xi \in L_b(x)\}} {|L_b(x)|}\)</span>
</li>
</ol>
<p><img src="grf_guide_files/figure-html/unnamed-chunk-2-1.png" width="700"></p>
<p>For a single tree and single split, Figure 1) shows the optimal split point in phase 1): it’s the vertical line that makes the left and right means of <span class="math inline">\(Y_i\)</span> as similar as possible (or equivalently, the squared difference in means as large as possible).</p>
<p>Figure 1) shows a target sample <span class="math inline">\(x\)</span> we wish to make a prediction for as a cross in the right-hand leaf. Phase 2) would average the <span class="math inline">\(Y_i\)</span>’s on this side, then repeat the same exercise for the rest of the <span class="math inline">\(B\)</span> trees, then average these to produce a final prediction. This procedure is a double summation, first over trees, then over training samples. We can swap the order of summation and obtain:</p>
<p><span class="math display">\[\begin{equation*}
\begin{split}
&amp; \hat \mu(x) = \sum_{i=1}^{n} \frac{1}{B} \sum_{b=1}^{B} Y_i \frac{1\{Xi \in L_b(x)\}} {|L_b(x)|} \\
&amp; = \sum_{i=1}^{n} Y_i \color{blue}{\alpha_i(x)},
\end{split}
\end{equation*}\]</span></p>
<p>where we have defined <span class="math inline">\(\color{blue}{\alpha_i(x)}\)</span> as the frequency with which the <span class="math inline">\(i\)</span>-th training sample falls into the same leaf as <span class="math inline">\(x\)</span>. That is, an equivalent description of a random forest’s prediction phase is that it predicts a weighted average of outcomes. These weights are the so-called random forest adaptive neighborhood weights, and they capture how “similar” a target sample <span class="math inline">\(x\)</span> is to each of the training samples <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1 \ldots n\)</span> (this weighting view of random forests appear frequently in statistical applications of random forests, see for example Zeileis, Hothorn &amp; Hornik (2003)).</p>
<p><strong>Causal Forest</strong> essentially combines Breiman (2001) and Robinson (1988) by modifying the steps above to:</p>
<ol style="list-style-type: decimal">
<li>Building phase: Greedily places covariate splits that maximize the squared difference in subgroup treatment effects <span class="math inline">\(n_L n_R (\hat \tau_L - \hat \tau_R)^2\)</span>, where <span class="math inline">\(\hat \tau\)</span> is obtained by running Robinson’s residual-on-residual regression for each possible split point<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</li>
<li>Use the resulting forest weights <span class="math inline">\(\color{blue}{\alpha(x)}\)</span> to estimate</li>
</ol>
<p><span class="math display">\[
\tau(x) := lm\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i), \text{weights} = \color{blue}{\alpha_i(x)} \Biggr).
\]</span> That is, <em>Causal Forest</em> is running a “forest”-localized version of Robinson’s regression. This adaptive weighting (instead of leaf-averaging) coupled with some other forest construction details known as “honesty” and “subsampling” can be used to give asymptotic guarantees for estimation and inference with random forests (Wager &amp; Athey, 2018). In the function <code>causal_forest</code>, the arguments <code>Y.hat</code> and <code>W.hat</code> supply estimates of <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span>, which by default are estimated with separate regression forests<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>The procedure we outlined above can be generalized (thus the name <em>generalized</em> random forest) to estimate other quantities besides heterogeneous treatment effects. The key ingredient is phase 1) above, by recognizing that Robinson’s regression equivalently can be expressed in “estimating equation” form given by a particular scoring function <span class="math inline">\(\psi_{\tau}(\cdot)\)</span>. GRF (Athey, Tibshirani &amp; Wager, 2019) generalizes phase 1) to target heterogeneity in some arbitrary parameter given as the solution to this estimating equation. That is, GRF essentially takes as input a formulation of how to estimate a certain parameter<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> <span class="math inline">\(\theta\)</span> <em>without</em> the presence of covariates (in the form of the scoring function <span class="math inline">\(\psi\)</span>) and then finds forest weights <span class="math inline">\(\alpha(x)\)</span> that yields <em>covariate</em>-specific estimates <span class="math inline">\(\theta(x)\)</span>.</p>
</div>
<div id="efficiently-estimating-summaries-of-the-cates" class="section level3">
<h3 class="hasAnchor">
<a href="#efficiently-estimating-summaries-of-the-cates" class="anchor"></a>Efficiently estimating summaries of the CATEs</h3>
<p>So far we have covered how Robinson’s residual-on-residual satisfied an orthogonality property in the sense that the target parameter <span class="math inline">\(\tau(x)\)</span> could be estimated reasonably well despite nuisance components (<span class="math inline">\(m(x)\)</span> and <span class="math inline">\(e(x)\)</span>) being estimated at slower rates. This is a desirable property of an estimating equation <span class="math inline">\(\psi\)</span>, particularly if we wish to estimate nuisance components with machine learning methods that, while more flexible than parametric models, converge at slower rates (Stone, 1980). What about estimating summaries of <span class="math inline">\(\tau(x)\)</span>, in terms of estimands like the average treatment effect (ATE), or the best linear projection (BLP), that have guaranteed root-n rate of convergence along with exact confidence intervals?</p>
<p>It turns out there are better approaches than simply averaging the <span class="math inline">\(\hat \tau(x)\)</span>’s obtained with a causal forest<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Robins, Rotnitzky &amp; Zhao (1994) showed that the so-called Augmented Inverse Probability Weighted (AIPW) estimator is asymptotically optimal for <span class="math inline">\(\tau\)</span> (meaning that among all non-parametric estimators, it has the lowest variance). It looks like the following:</p>
<p><span class="math display">\[
\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n}\left( \mu(X_i, 1) - \mu(X_i, 0) + W_i \frac{Y_i - \mu(X_i, 1)}{e(X_i)} - (1 - W_i)\frac{Y_i - \mu(X_i, 0)}{1 - e(X_i)} \right),
\]</span> where <span class="math inline">\(\mu(X_i, 1), \mu(X_i, 0)\)</span> are estimates of the conditional means in the two treatment arms: <span class="math inline">\(\mu(X_i, W_i) = E[Y_i | X_i = x, W_i = w]\)</span>. This expression can be rearranged and expressed as</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n}\left( \tau(X_i) + \frac{W_i - e(X_i)}{e(X_i)[1 - e(X_i)]} \left(Y_i - \mu(X_i, W_i)\right) \right) \\
= \frac{1}{n} \sum_{i=1}^{n} \Gamma_i.
\]</span> We can identify the three terms as 1) an initial treatment effect estimate <span class="math inline">\(\tau(X_i)\)</span>, 2) a debiasing weight <span class="math inline">\(\frac{W_i - e(X_i)}{e(X_i)[1 - e(X_i)]}\)</span>, and 3) a residual <span class="math inline">\(Y_i - \mu(X_i, W_i)\)</span>. The asymptotic efficiency property of this plug-in construction continues to hold if we substitute in non-parametric nuisance estimates obtained with causal forest (subject to some regularity conditions such as cross-fitting, Zheng &amp; van der Laan (2011), Chernozhukov et al., (2018)).</p>
<p>In general, the construction above gives rise to so-called <strong>doubly robust</strong> estimating equations, and it is the approach taken by <code>grf</code> when computing summary measures such as the ATE, BLP, and “RATE”. The associated doubly robust scores <span class="math inline">\(\hat \Gamma_i\)</span> are accessible through the function <code>get_scores</code>. As a concrete example, the ATE is an average of <span class="math inline">\(\hat \Gamma_i\)</span>’s, while the best linear projection is a regression of the <span class="math inline">\(\hat \Gamma_i\)</span>’s on a set of covariates <span class="math inline">\(X_i\)</span>. Summary measures built on these play a crucial role in interpreting non-parametric CATE estimates, as they are by the fundamental limits of non-parametric estimation (machine learning included), <em>noisy</em> point estimates. The next two sections walk through empirical examples.</p>
</div>
</div>
<div id="application-school-program-evaluation" class="section level2">
<h2 class="hasAnchor">
<a href="#application-school-program-evaluation" class="anchor"></a>Application: school program evaluation</h2>
<p>In this section, we walk through an example application of GRF. The data we are using is from Bruhn et al. (2016), which conducted an RCT in Brazil in which high schools were randomly assigned a financial education program (in settings like this it is common to randomize at the school level to avoid student-level interference). This program increased student financial proficiency on average. Other outcomes are considered in the paper, we’ll focus on the financial proficiency score here. A processed copy of this data, containing student-level data from around 17 000 students, is stored on the <a href="https://github.com/grf-labs/grf/tree/master/r-package/grf/vignettes/data/">github repo</a>, it extracts basic student characteristics, as well as additional baseline survey responses we use as covariates (two of these are aggregated into an index by the authors to assess student’s ability to save, and their financial autonomy).</p>
<p><em>Notation</em>: throughout we use variable names <code>Y</code> and <code>W</code> to denote outcomes and binary treatment assignment, and <code>Y.hat</code> to denote estimates of <span class="math inline">\(E[Y_i | X_i = x]\)</span>, <code>W.hat</code> estimates of <span class="math inline">\(E[W_i | X_i = x]\)</span>, and <code>tau.hat</code> estimates of <span class="math inline">\(\tau(X_i) = E[Y_i(1) - Y_i(0) | X_i = x]\)</span>.</p>
<div id="data-overview" class="section level3">
<h3 class="hasAnchor">
<a href="#data-overview" class="anchor"></a>Data overview</h3>
<div class="sourceCode" id="cb2"><html><body><pre class="r"><span class="no">data</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span>(<span class="st">"data/bruhn2016.csv"</span>)
<span class="no">Y</span> <span class="kw">&lt;-</span> <span class="no">data</span>$<span class="no">outcome</span>
<span class="no">W</span> <span class="kw">&lt;-</span> <span class="no">data</span>$<span class="no">treatment</span>
<span class="no">school</span> <span class="kw">&lt;-</span> <span class="no">data</span>$<span class="no">school</span>
<span class="no">X</span> <span class="kw">&lt;-</span> <span class="no">data</span>[-(<span class="fl">1</span>:<span class="fl">3</span>)]

<span class="co"># Around 30% have one or more missing covariates, the missingness pattern doesn't seem</span>
<span class="co"># to vary systematically between the treated and controls, so we'll keep them in the analysis</span>
<span class="co"># since GRF supports splitting on X's with missing values.</span>
<span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span>(!<span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span>(<span class="no">X</span>)) / <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span>(<span class="no">X</span>)
<span class="co">#&gt; [1] 0.29</span>
<span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span>(<span class="no">W</span> ~ !<span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span>(<span class="no">X</span>))
<span class="co">#&gt; </span>
<span class="co">#&gt;  Welch Two Sample t-test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  W by !complete.cases(X)</span>
<span class="co">#&gt; t = -0.4, df = 9490, p-value = 0.7</span>
<span class="co">#&gt; alternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0</span>
<span class="co">#&gt; 95 percent confidence interval:</span>
<span class="co">#&gt;  -0.020  0.013</span>
<span class="co">#&gt; sample estimates:</span>
<span class="co">#&gt; mean in group FALSE  mean in group TRUE </span>
<span class="co">#&gt;                0.51                0.52</span></pre></body></html></div>
</div>
<div id="estimating-and-summarizing-cates" class="section level3">
<h3 class="hasAnchor">
<a href="#estimating-and-summarizing-cates" class="anchor"></a>Estimating and summarizing CATEs</h3>
<p>Throughout we’ll fix the propensity score to <code>W.hat = 0.5</code> since we know this is an RCT (otherwise we’d fit a propensity model for <code>W.hat</code>, and inspect the histogram of estimated probabilities to assess how plausible the overlap assumption is).</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="no">cf</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/causal_forest.html">causal_forest</a></span>(<span class="no">X</span>, <span class="no">Y</span>, <span class="no">W</span>, <span class="kw">W.hat</span> <span class="kw">=</span> <span class="fl">0.5</span>, <span class="kw">clusters</span> <span class="kw">=</span> <span class="no">school</span>)</pre></body></html></div>
<p>Compute a doubly robust ATE estimate.</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r"><span class="no">ate</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/average_treatment_effect.html">average_treatment_effect</a></span>(<span class="no">cf</span>)
<span class="no">ate</span>
<span class="co">#&gt; estimate  std.err </span>
<span class="co">#&gt;     4.35     0.52</span></pre></body></html></div>
<p>The benefit appears to be quite strong compared to the overall outcome scale: <code>ate[1] / sd(Y)</code>= 0.3, and is in line with Bruhn et al. (2016).</p>
<p>A very simple way to see which variables appear to make a difference for treatment effects is to inspect <code>variable_importance</code>, which measures how often a variable <span class="math inline">\(X_j\)</span> was split on.</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="no">varimp</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/variable_importance.html">variable_importance</a></span>(<span class="no">cf</span>)
<span class="no">ranked.vars</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span>(<span class="no">varimp</span>, <span class="kw">decreasing</span> <span class="kw">=</span> <span class="fl">TRUE</span>)

<span class="co"># Top 5 variables according to this measure</span>
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">X</span>)[<span class="no">ranked.vars</span>[<span class="fl">1</span>:<span class="fl">5</span>]]
<span class="co">#&gt; [1] "financial.autonomy.index"           "intention.to.save.index"           </span>
<span class="co">#&gt; [3] "family.receives.cash.transfer"      "has.computer.with.internet.at.home"</span>
<span class="co">#&gt; [5] "is.female"</span></pre></body></html></div>
<p>An easy to interpret summary measure of the CATEs is the <a href="https://grf-labs.github.io/grf/reference/best_linear_projection.html">best linear projection</a>, which provides a doubly robust estimate of the linear model</p>
<p><span class="math display">\[\tau(X_i) = \beta_0 + A_i \beta,\]</span> where <span class="math inline">\(A_i\)</span> is a vector of covariates. If we set <span class="math inline">\(A\)</span> to the covariates with the highest variable importance we can estimate a simple linear association measure for the CATEs:</p>
<div class="sourceCode" id="cb6"><html><body><pre class="r"><span class="fu"><a href="../reference/best_linear_projection.html">best_linear_projection</a></span>(<span class="no">cf</span>, <span class="no">X</span>[<span class="no">ranked.vars</span>[<span class="fl">1</span>:<span class="fl">5</span>]])
<span class="co">#&gt; </span>
<span class="co">#&gt; Best linear projection of the conditional average treatment effect.</span>
<span class="co">#&gt; Confidence intervals are cluster- and heteroskedasticity-robust (HC3):</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;                                    Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)                          7.3364     1.0807    6.79  1.2e-11 ***</span>
<span class="co">#&gt; financial.autonomy.index            -0.0244     0.0140   -1.74    0.082 .  </span>
<span class="co">#&gt; intention.to.save.index             -0.0124     0.0156   -0.80    0.426    </span>
<span class="co">#&gt; family.receives.cash.transfer       -0.8700     0.6363   -1.37    0.172    </span>
<span class="co">#&gt; has.computer.with.internet.at.home  -0.8212     0.6264   -1.31    0.190    </span>
<span class="co">#&gt; is.female                           -0.8019     0.5356   -1.50    0.134    </span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></pre></body></html></div>
<p>Looking at the best linear projection (BLP) it appears students with a high “financial autonomy index” benefits less from treatment, the RCT authors write that this is a “psychology-based financial autonomy index that aggregated a series of questions that measured whether students felt empowered, confident, and capable of making independent financial decisions and influencing the financial decisions of the households”. The BLP appears to suggest that students that already are financially comfortable as measured by this index, don’t benefit much from the training course.</p>
</div>
<div id="evaluating-cate-estimates-with-rate" class="section level3">
<h3 class="hasAnchor">
<a href="#evaluating-cate-estimates-with-rate" class="anchor"></a>Evaluating CATE estimates with RATE</h3>
<p>Causal inference is fundamentally more challenging than the typical predictive use of machine learning algorithms that have a well-defined scoring metric, such as a prediction error. Treatment effects are fundamentally unobserved, so we need alternative metrics to assess performance. The <em>R-loss</em> discussed in Nie &amp; Wager (2021) is one such metric, and could, for example, be used as a cross-validation criterion, however, it does not tell us anything about whether there are HTEs present. Even though the true treatment effects are unobserved, we can use suitable <em>estimates</em> of treatment effects on held out data to evaluate models. The <em>Rank-Weighted Average Treatment Effect</em> (<a href="https://grf-labs.github.io/grf/reference/rank_average_treatment_effect.html">RATE</a>) (Yadlowsky et al., 2022) is a metric that assesses how well a CATE estimator does in ranking units according to estimated treatment benefit. It can be thought of as an Area Under the Curve (AUC) measure for heterogeneity, where a larger number is better.</p>
<p>The RATE has an appealing visual component, in that it is the area under the curve that traces out the following difference in expected values while varying the treated fraction <span class="math inline">\(q \in [0, 1]\)</span>: <span class="math display">\[TOC(q) = E[Y_i(1) - Y_i(0) | \hat \tau(X_i) \geq F^{-1}_{\hat \tau(X_i)}(1 - q)] - E[Y_i(1) - Y_i(0)],\]</span> (<span class="math inline">\(F(\cdot)\)</span> is the distribution function). I.e. at <span class="math inline">\(q = 0.2\)</span> the TOC quantifies the incremental benefit of treating only the 20% with the largest estimated CATEs compared to the overall ATE. We refer to the area under the TOC as the <em>AUTOC</em>, there are other RATE metrics such as the <em>Qini</em>, which weights the area under the TOC differently.</p>
<p>A recipe for valid inference with RATE is to split the data into a training and evaluation set, one for evaluating the RATE, and another to train a CATE estimator. A drawback of this approach is that we may detect different findings depending on the random sample used to define the evaluation and training sets. Since the RCT was clustered at the school level, we’ll draw random units at the school level:</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="no">samples.by.school</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/split.html">split</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span>(<span class="no">school</span>), <span class="no">school</span>)
<span class="no">num.schools</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span>(<span class="no">samples.by.school</span>)
<span class="no">train</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span>(<span class="no">samples.by.school</span>[<span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span>(<span class="fl">1</span>:<span class="no">num.schools</span>, <span class="no">num.schools</span> / <span class="fl">2</span>)])

<span class="no">train.forest</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/causal_forest.html">causal_forest</a></span>(<span class="no">X</span>[<span class="no">train</span>, ], <span class="no">Y</span>[<span class="no">train</span>], <span class="no">W</span>[<span class="no">train</span>], <span class="kw">W.hat</span> <span class="kw">=</span> <span class="fl">0.5</span>, <span class="kw">clusters</span> <span class="kw">=</span> <span class="no">school</span>[<span class="no">train</span>])
<span class="no">tau.hat.eval</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">train.forest</span>, <span class="no">X</span>[-<span class="no">train</span>, ])$<span class="no">predictions</span>

<span class="no">eval.forest</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/causal_forest.html">causal_forest</a></span>(<span class="no">X</span>[-<span class="no">train</span>, ], <span class="no">Y</span>[-<span class="no">train</span>], <span class="no">W</span>[-<span class="no">train</span>], <span class="kw">W.hat</span> <span class="kw">=</span> <span class="fl">0.5</span>, <span class="kw">clusters</span> <span class="kw">=</span> <span class="no">school</span>[-<span class="no">train</span>])

<span class="no">rate.cate</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a></span>(<span class="no">eval.forest</span>, <span class="no">tau.hat.eval</span>)
<span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">rate.cate</span>, <span class="kw">main</span> <span class="kw">=</span> <span class="st">"TOC: By decreasing estimated CATE"</span>)</pre></body></html></div>
<p><img src="grf_guide_files/figure-html/unnamed-chunk-8-1.png" width="700"></p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"><span class="no">rate.cate</span>
<span class="co">#&gt;  estimate std.err             target</span>
<span class="co">#&gt;      0.35    0.33 priorities | AUTOC</span></pre></body></html></div>
<p>For this data set in particular, whether the RATE is significant or not depends on if the CATE estimator is trained on a subsample with sufficient signal (training models for <code>Y.hat</code> using different methods, such as boosting or penalized linear models, may sometimes be useful). There are more ways to utilize the RATE metric besides CATE rankings, we can compute the TOC by for example the ranking of some specific variable. The previous section suggested that students with high financial autonomy benefit less from the program. We can use the RATE to evaluate how prioritizing students based on this index does:</p>
<div class="sourceCode" id="cb9"><html><body><pre class="r"><span class="co"># We use the trained forest on the full data since we are computing the RATE based on a covariate.</span>
<span class="no">rate.fin.index</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a></span>(
  <span class="no">cf</span>,
  -<span class="fl">1</span> * <span class="no">X</span>$<span class="no">financial.autonomy.index</span>, <span class="co"># Multiply by -1 to order by decreasing index</span>
  <span class="kw">subset</span> <span class="kw">=</span> !<span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span>(<span class="no">X</span>$<span class="no">financial.autonomy.index</span>) <span class="co"># Ignore missing X-values</span>
  )
<span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">rate.fin.index</span>, <span class="kw">xlab</span> <span class="kw">=</span> <span class="st">"Treated fraction"</span>, <span class="kw">ylab</span> <span class="kw">=</span> <span class="st">"Increase in test scores"</span>, <span class="kw">main</span> <span class="kw">=</span> <span class="st">"TOC: By increasing financial autonomy"</span>)</pre></body></html></div>
<p><img src="grf_guide_files/figure-html/unnamed-chunk-9-1.png" width="700"></p>
<div class="sourceCode" id="cb10"><html><body><pre class="r"><span class="no">rate.fin.index</span>
<span class="co">#&gt;  estimate std.err             target</span>
<span class="co">#&gt;      0.67    0.22 priorities | AUTOC</span></pre></body></html></div>
<p>The TOC suggests that students with a lower financial autonomy index benefits more than average from the program, and AUTOC is significant at conventional levels: <code>rate.fin.index$estimate / rate.fin.index$std.err</code> = 3.01. Compared to the standard deviation of the outcome <code><a href="https://rdrr.io/r/stats/sd.html">sd(Y)</a></code> = 14.74 the benefit also appears meaningful.</p>
</div>
<div id="tree-based-policy-learning" class="section level3">
<h3 class="hasAnchor">
<a href="#tree-based-policy-learning" class="anchor"></a>Tree-based policy learning</h3>
<p>In this section, we’ll walk through finding a tree-based treatment policy using the GRF sister package <a href="https://github.com/grf-labs/policytree">policytree</a>. Since the RATE suggests that some parts of the population appear to benefit less from the education program, such as those students that are already highly “financially autonomous”, we’ll try to see if we can use a data adaptive algorithm to assign the treatment based on subgroups. One way to do this is to posit that a simple and interpretable covariate based rule should determine program participation, this is what <code>policytree</code> does, by finding a shallow decision tree that maximizes the empirical “reward” of adhering to this treatment assignment policy.</p>
<p>Denote by <span class="math inline">\(\pi(X_i)\)</span> a function (policy) that takes a covariate vector <span class="math inline">\(X_i\)</span> and maps it to a binary decision of whether the subject should be treated or not: <span class="math inline">\(\pi \mapsto \{0, 1\}\)</span> (this can easily be extended to multiple treatments). The task of finding such a policy is referred to as policy learning. Given empirical “reward scores” <span class="math inline">\(\hat \Gamma_i\)</span>, <code>policytree</code> finds a <em>tree-based</em> policy <span class="math inline">\(\pi(X_i)\)</span> that maximizes</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^{n} (2\pi(X_i) - 1)\hat \Gamma_i.\]</span> (note on where “2” comes from: since <span class="math inline">\(\pi \in \{0, 1\}\)</span> then <span class="math inline">\(2\pi - 1 \in \{-1, 1\}\)</span>, i.e. the objective function increases by <span class="math inline">\(\hat \Gamma_i\)</span> when we give unit <span class="math inline">\(i\)</span> the treatment and decrease by <span class="math inline">\(-\hat \Gamma_i\)</span> when we do not give unit <span class="math inline">\(i\)</span> the treatment).</p>
<p>Below we’ll walk through an example of using the <a href="https://grf-labs.github.io/policytree/reference/policy_tree.html">policy_tree</a> function from <code>policytree</code> to find an optimal depth-2 decision tree policy. This function uses a C++ solver to find a shallow and optimal decision rule<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. We are using separate data sets to fit and evaluate the policy:</p>
<div class="sourceCode" id="cb11"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">policytree</span>)
<span class="co"># Use train/evaluation school split from above, but use non-missing units for policy_tree</span>
<span class="no">eval</span> <span class="kw">&lt;-</span> (<span class="fl">1</span>:<span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span>(<span class="no">X</span>))[-<span class="no">train</span>]
<span class="no">not.missing</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span>(<span class="no">X</span>))
<span class="no">train</span> <span class="kw">&lt;-</span> <span class="no">train</span>[<span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span>(<span class="no">train</span> <span class="kw">%in%</span> <span class="no">not.missing</span>)]
<span class="no">eval</span> <span class="kw">&lt;-</span> <span class="no">eval</span>[<span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span>(<span class="no">eval</span> <span class="kw">%in%</span> <span class="no">not.missing</span>)]

<span class="co"># Compute doubly robust scores</span>
<span class="no">dr.scores</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/get_scores.html">get_scores</a></span>(<span class="no">cf</span>)
<span class="co"># Use as the ATE as a "cost" of program treatment to find something non-trivial</span>
<span class="no">cost</span> <span class="kw">&lt;-</span> <span class="no">ate</span><span class="kw">[[</span><span class="st">"estimate"</span>]]
<span class="no">dr.rewards</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(<span class="kw">control</span><span class="kw">=</span>-<span class="no">dr.scores</span>, <span class="kw">treat</span><span class="kw">=</span><span class="no">dr.scores</span> - <span class="no">cost</span>)

<span class="co"># Fit depth 2 tree on training subset</span>
<span class="no">tree</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/policytree/man/policy_tree.html">policy_tree</a></span>(<span class="no">X</span>[<span class="no">train</span>, ], <span class="no">dr.rewards</span>[<span class="no">train</span>, ], <span class="kw">min.node.size</span> <span class="kw">=</span> <span class="fl">100</span>)
<span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">tree</span>, <span class="kw">leaf.labels</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"dont treat"</span>, <span class="st">"treat"</span>))</pre></body></html></div>
<div class="grViz html-widget html-fill-item" id="htmlwidget-16b7ee6a87b0be144780" style="width:700px;height:432.632880098888px;"></div>
<script type="application/json" data-for="htmlwidget-16b7ee6a87b0be144780">{"x":{"diagram":"digraph nodes { \n node [shape=box] ;\n0 [label=\" family.receives.cash.transfer <= 0 \"] ;\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"];\n0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"]\n1  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\" treat \"];\n2 [label=\" makes.list.of.expenses.every.month <= 0 \"] ;\n2 -> 3  ;\n2 -> 4  ;\n3  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\" treat \"];\n4  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\" dont treat \"];\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script><p>The fitted rule can evidently discriminate between units that benefit more than average from the program in-sample. To assess the statistical validity of this we should evaluate it on a held out data set, where it won’t necessarily give the same mean reward:</p>
<div class="sourceCode" id="cb12"><html><body><pre class="r"><span class="no">pi.hat</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">tree</span>, <span class="no">X</span>[<span class="no">eval</span>,]) - <span class="fl">1</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span>((<span class="no">dr.scores</span>[<span class="no">eval</span>] - <span class="no">cost</span>) * (<span class="fl">2</span>*<span class="no">pi.hat</span> - <span class="fl">1</span>))
<span class="co">#&gt; [1] -0.28</span></pre></body></html></div>
<p>We can decompose the ATE into the subgroups implied by the tree, and look at some summary statistics on the held out sample, such as the ATE minus cost:</p>
<div class="sourceCode" id="cb13"><html><body><pre class="r"><span class="no">leaf.node</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">tree</span>, <span class="no">X</span>[<span class="no">eval</span>, ], <span class="kw">type</span> <span class="kw">=</span> <span class="st">"node.id"</span>)
<span class="no">action.by.leaf</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/split.html">split</a></span>(<span class="no">pi.hat</span>, <span class="no">leaf.node</span>), <span class="no">unique</span>))

<span class="co"># Define a function to compute summary statistics</span>
<span class="no">leaf_stats</span> <span class="kw">&lt;-</span> <span class="kw">function</span>(<span class="no">leaf</span>) {
  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="kw">ATE.minus.cost</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span>(<span class="no">leaf</span>), <span class="kw">std.err</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span>(<span class="no">leaf</span>) / <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span>(<span class="no">leaf</span>)), <span class="kw">size</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span>(<span class="no">leaf</span>))
}

<span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(
  <span class="kw">rule</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span>(<span class="no">action.by.leaf</span> <span class="kw">==</span> <span class="fl">0</span>, <span class="st">"dont treat"</span>, <span class="st">"treat"</span>),
  <span class="fu"><a href="https://rdrr.io/r/stats/aggregate.html">aggregate</a></span>(<span class="no">dr.scores</span>[<span class="no">eval</span>] - <span class="no">cost</span>, <span class="kw">by</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">leaf.node</span> <span class="kw">=</span> <span class="no">leaf.node</span>),
            <span class="kw">FUN</span> <span class="kw">=</span> <span class="no">leaf_stats</span>)
)
<span class="co">#&gt;         rule leaf.node x.ATE.minus.cost x.std.err  x.size</span>
<span class="co">#&gt; 2      treat         2            -0.24      0.43 4035.00</span>
<span class="co">#&gt; 4      treat         4            -0.83      0.63 1787.00</span>
<span class="co">#&gt; 5 dont treat         5            -3.25      1.81  238.00</span></pre></body></html></div>
<p>A depth-2 tree might be a big ask, if we instead ask for just a single split (treat/don’t treat based on a single covariate cutoff), we get:</p>
<div class="sourceCode" id="cb14"><html><body><pre class="r"><span class="no">tree.depth1</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/policytree/man/policy_tree.html">policy_tree</a></span>(<span class="no">X</span>[<span class="no">train</span>, ], <span class="no">dr.rewards</span>[<span class="no">train</span>, ], <span class="kw">depth</span> <span class="kw">=</span> <span class="fl">1</span>, <span class="kw">min.node.size</span> <span class="kw">=</span> <span class="fl">100</span>)
<span class="no">pi.hat.eval</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">tree.depth1</span>, <span class="no">X</span>[<span class="no">eval</span>, ]) - <span class="fl">1</span>

<span class="no">treat.eval</span> <span class="kw">&lt;-</span> <span class="no">eval</span>[<span class="no">pi.hat.eval</span> <span class="kw">==</span> <span class="fl">1</span>]
<span class="no">dont.treat.eval</span> <span class="kw">&lt;-</span> <span class="no">eval</span>[<span class="no">pi.hat.eval</span> <span class="kw">==</span> <span class="fl">0</span>]

<span class="fu"><a href="../reference/average_treatment_effect.html">average_treatment_effect</a></span>(<span class="no">cf</span>, <span class="kw">subset</span> <span class="kw">=</span> <span class="no">treat.eval</span>)
<span class="co">#&gt; estimate  std.err </span>
<span class="co">#&gt;      3.9      0.8</span>
<span class="fu"><a href="../reference/average_treatment_effect.html">average_treatment_effect</a></span>(<span class="no">cf</span>, <span class="kw">subset</span> <span class="kw">=</span> <span class="no">dont.treat.eval</span>)
<span class="co">#&gt; estimate  std.err </span>
<span class="co">#&gt;      1.9      2.3</span></pre></body></html></div>
<div class="sourceCode" id="cb15"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">tree.depth1</span>, <span class="kw">leaf.labels</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"dont treat"</span>, <span class="st">"treat"</span>))</pre></body></html></div>
<div class="grViz html-widget html-fill-item" id="htmlwidget-a160f28a6ca1cb08d37c" style="width:700px;height:432.632880098888px;"></div>
<script type="application/json" data-for="htmlwidget-a160f28a6ca1cb08d37c">{"x":{"diagram":"digraph nodes { \n node [shape=box] ;\n0 [label=\" financial.autonomy.index <= 84 \"] ;\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"];\n0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"]\n1  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\" treat \"];\n2  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\" dont treat \"];\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="application-measuring-the-effect-of-poverty-on-attention" class="section level2">
<h2 class="hasAnchor">
<a href="#application-measuring-the-effect-of-poverty-on-attention" class="anchor"></a>Application: measuring the effect of poverty on attention</h2>
<p>In this section, we walk through an example following Farbmacher et al. (2021) who studies a causal forest application using data from Carvalho et al. (2016). Carvalho et al. (2016) conducted an experiment where they randomly assigned low-income individuals to perform a cognitive test before (<span class="math inline">\(W=1\)</span>) or after (<span class="math inline">\(W=0\)</span>) payday. The outcome is the performance on a test designed to measure cognitive ability. The original study did not find an effect on average, however, Farbmacher et al. (2021) conducts an HTE analysis and finds evidence suggesting there are parts of the population where poverty impairs cognitive function. A processed copy of this data is stored on the <a href="https://github.com/grf-labs/grf/tree/master/r-package/grf/vignettes/data">github repo</a> and contains observations from around 2 500 individuals along with 27 characteristics such as age, income, etc. Education is stored as an ordinal value where a high value means high education and low less education (4: college graduate, 3: some college, 2: high school graduate, 1: less than high school).</p>
<div id="data-overview-1" class="section level3">
<h3 class="hasAnchor">
<a href="#data-overview-1" class="anchor"></a>Data overview</h3>
<div class="sourceCode" id="cb16"><html><body><pre class="r"><span class="no">data</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span>(<span class="st">"data/carvalho2016.csv"</span>)
<span class="co"># As outcomes we'll look at the number of correct answers.</span>
<span class="no">Y</span> <span class="kw">&lt;-</span> <span class="no">data</span>$<span class="no">outcome.num.correct.ans</span>
<span class="no">W</span> <span class="kw">&lt;-</span> <span class="no">data</span>$<span class="no">treatment</span>
<span class="no">X</span> <span class="kw">&lt;-</span> <span class="no">data</span>[-(<span class="fl">1</span>:<span class="fl">4</span>)]</pre></body></html></div>
<p>The study was reported to be conducted as an RCT, just to be sure we estimate the propensity score to verify we might not unintentionally miss a lack of overlap:</p>
<div class="sourceCode" id="cb17"><html><body><pre class="r"><span class="no">rf</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/regression_forest.html">regression_forest</a></span>(<span class="no">X</span>, <span class="no">W</span>, <span class="kw">num.trees</span> <span class="kw">=</span> <span class="fl">500</span>)
<span class="no">p.hat</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">rf</span>)$<span class="no">predictions</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span>(<span class="no">p.hat</span>)</pre></body></html></div>
<p><img src="grf_guide_files/figure-html/unnamed-chunk-16-1.png" width="700"></p>
<p>The data contains 2 480 observations of 24 covariates. Fitting at CATE estimator on such a large amount of covariates relative to the sample size is likely to be underpowered in HTE detection, so below we first predict the conditional mean <span class="math inline">\(E[Y_i | X_i]\)</span>, then use a simple forest variable importance to choose which covariates to keep in the CATE estimation step.</p>
<div class="sourceCode" id="cb18"><html><body><pre class="r"><span class="no">Y.forest</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/regression_forest.html">regression_forest</a></span>(<span class="no">X</span>, <span class="no">Y</span>)
<span class="no">Y.hat</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">Y.forest</span>)$<span class="no">predictions</span>

<span class="no">varimp.Y</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/variable_importance.html">variable_importance</a></span>(<span class="no">Y.forest</span>)

<span class="co"># Keep the top 10 variables for CATE estimation</span>
<span class="no">keep</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">X</span>)[<span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span>(<span class="no">varimp.Y</span>, <span class="kw">decreasing</span> <span class="kw">=</span> <span class="fl">TRUE</span>)[<span class="fl">1</span>:<span class="fl">10</span>]]
<span class="no">keep</span>
<span class="co">#&gt;  [1] "education"                         "black"                            </span>
<span class="co">#&gt;  [3] "white"                             "household.income"                 </span>
<span class="co">#&gt;  [5] "age"                               "disabled"                         </span>
<span class="co">#&gt;  [7] "pay.day.amount.fraction.of.income" "current.income"                   </span>
<span class="co">#&gt;  [9] "hispanic"                          "household.size"</span></pre></body></html></div>
</div>
<div id="analyzing-heterogeneity-with-the-toc" class="section level3">
<h3 class="hasAnchor">
<a href="#analyzing-heterogeneity-with-the-toc" class="anchor"></a>Analyzing heterogeneity with the TOC</h3>
<div class="sourceCode" id="cb19"><html><body><pre class="r"><span class="no">X.cf</span> <span class="kw">&lt;-</span> <span class="no">X</span>[, <span class="no">keep</span>]
<span class="no">W.hat</span> <span class="kw">&lt;-</span> <span class="fl">0.5</span>

<span class="co"># Set aside the first half of the data for training and the second for evaluation.</span>
<span class="co"># (Note that the results may change depending on which samples we hold out for training/evaluation)</span>
<span class="no">train</span> <span class="kw">&lt;-</span> <span class="fl">1</span>:(<span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span>(<span class="no">X.cf</span>) / <span class="fl">2</span>)

<span class="no">train.forest</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/causal_forest.html">causal_forest</a></span>(<span class="no">X.cf</span>[<span class="no">train</span>, ], <span class="no">Y</span>[<span class="no">train</span>], <span class="no">W</span>[<span class="no">train</span>], <span class="kw">Y.hat</span> <span class="kw">=</span> <span class="no">Y.hat</span>[<span class="no">train</span>], <span class="kw">W.hat</span> <span class="kw">=</span> <span class="no">W.hat</span>)
<span class="no">tau.hat.eval</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">train.forest</span>, <span class="no">X.cf</span>[-<span class="no">train</span>, ])$<span class="no">predictions</span>

<span class="no">eval.forest</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/causal_forest.html">causal_forest</a></span>(<span class="no">X.cf</span>[-<span class="no">train</span>, ], <span class="no">Y</span>[-<span class="no">train</span>], <span class="no">W</span>[-<span class="no">train</span>], <span class="kw">Y.hat</span> <span class="kw">=</span> <span class="no">Y.hat</span>[-<span class="no">train</span>], <span class="kw">W.hat</span> <span class="kw">=</span> <span class="no">W.hat</span>)</pre></body></html></div>
<p>In line with the study mentioned above, there does not appear to be an effect on average:</p>
<div class="sourceCode" id="cb20"><html><body><pre class="r"><span class="fu"><a href="../reference/average_treatment_effect.html">average_treatment_effect</a></span>(<span class="no">eval.forest</span>)
<span class="co">#&gt; estimate  std.err </span>
<span class="co">#&gt;     0.17     0.60</span></pre></body></html></div>
<div class="sourceCode" id="cb21"><html><body><pre class="r"><span class="no">varimp</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/variable_importance.html">variable_importance</a></span>(<span class="no">eval.forest</span>)
<span class="no">ranked.vars</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span>(<span class="no">varimp</span>, <span class="kw">decreasing</span> <span class="kw">=</span> <span class="fl">TRUE</span>)
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">X.cf</span>)[<span class="no">ranked.vars</span>[<span class="fl">1</span>:<span class="fl">5</span>]]
<span class="co">#&gt; [1] "age"              "current.income"   "education"        "household.income"</span>
<span class="co">#&gt; [5] "household.size"</span></pre></body></html></div>
<p>The outcome is the number of correct answers on a test measuring cognitive ability, so a negative CATE means an impairment in cognition due to poverty, so in computing the RATE (AUTOC) we rank by the most negative CATEs. We also consider age (old to young), as it shows up high in the split frequency.</p>
<div class="sourceCode" id="cb22"><html><body><pre class="r"><span class="no">rate.cate</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a></span>(<span class="no">eval.forest</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">cate</span> <span class="kw">=</span> -<span class="fl">1</span> *<span class="no">tau.hat.eval</span>))
<span class="no">rate.age</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a></span>(<span class="no">eval.forest</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">age</span> <span class="kw">=</span> <span class="no">X</span>[-<span class="no">train</span>, <span class="st">"age"</span>]))

<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span>(<span class="kw">mfrow</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">1</span>, <span class="fl">2</span>))
<span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">rate.cate</span>, <span class="kw">ylab</span> <span class="kw">=</span> <span class="st">"Number of correct answers"</span>, <span class="kw">main</span> <span class="kw">=</span> <span class="st">"TOC: By most negative CATEs"</span>)
<span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">rate.age</span>, <span class="kw">ylab</span> <span class="kw">=</span> <span class="st">"Number of correct answers"</span>, <span class="kw">main</span> <span class="kw">=</span> <span class="st">"TOC: By decreasing age"</span>)</pre></body></html></div>
<p><img src="grf_guide_files/figure-html/unnamed-chunk-21-1.png" width="700"></p>
<div class="sourceCode" id="cb23"><html><body><pre class="r">
<span class="no">rate.cate</span>
<span class="co">#&gt;  estimate std.err       target</span>
<span class="co">#&gt;      -1.6    0.62 cate | AUTOC</span>
<span class="no">rate.age</span>
<span class="co">#&gt;  estimate std.err      target</span>
<span class="co">#&gt;      -1.1    0.69 age | AUTOC</span></pre></body></html></div>
<p>The TOC is informative here since it appears to be indicating a lower quantile <span class="math inline">\(q\)</span> where there may be a decline in cognition. Farbmacher et al. (2021) argue that there appears to be a small subset of the population experiencing adverse effects from poverty, and this is a setting where the AUTOC can be more powerful than the so-called Qini coefficient in detecting heterogeneity. Computing the same evaluations using the Qini weighting yields lower estimates.</p>
<div class="sourceCode" id="cb24"><html><body><pre class="r"><span class="no">qini.cate</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a></span>(<span class="no">eval.forest</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">cate</span> <span class="kw">=</span> -<span class="fl">1</span> *<span class="no">tau.hat.eval</span>), <span class="kw">target</span> <span class="kw">=</span> <span class="st">"QINI"</span>)
<span class="no">qini.age</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/rank_average_treatment_effect.html">rank_average_treatment_effect</a></span>(<span class="no">eval.forest</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">age</span> <span class="kw">=</span> <span class="no">X</span>[-<span class="no">train</span>, <span class="st">"age"</span>]), <span class="kw">target</span> <span class="kw">=</span> <span class="st">"QINI"</span>)

<span class="no">qini.cate</span>
<span class="co">#&gt;  estimate std.err      target</span>
<span class="co">#&gt;     -0.29    0.18 cate | QINI</span>
<span class="no">qini.age</span>
<span class="co">#&gt;  estimate std.err     target</span>
<span class="co">#&gt;     -0.27    0.17 age | QINI</span></pre></body></html></div>
</div>
</div>
<div id="further-resources" class="section level2">
<h2 class="hasAnchor">
<a href="#further-resources" class="anchor"></a>Further resources</h2>
<p><em>Some educational resources</em></p>
<ul>
<li><p><a href="https://www.youtube.com/playlist?list=PLxq_lXOUlvQAoWZEqhRqHNezS30lI49G-">Stanford video lectures on machine learning &amp; causal inference</a></p></li>
<li><p><a href="https://youtu.be/YBbnCDRCcAI">Online seminar on estimating heterogeneous treatment effects in R</a></p></li>
<li><p><a href="http://web.stanford.edu/~swager/stats361.pdf">Lecture notes - causal inference PhD course at Stanford</a></p></li>
</ul>
<center>
<img src="https://raw.githubusercontent.com/grf-labs/grf/master/images/logo/grf_leaf_green.png" height="64">
</center>
</div>
<div id="references" class="section level2">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<p>Athey, Susan, Julie Tibshirani, and Stefan Wager. “Generalized random forests.” The Annals of Statistics 47, no. 2 (2019): 1148-1178.</p>
<p>Breiman, Leo. “Random forests.” Machine learning 45, no. 1 (2001): 5-32.</p>
<p>Bruhn, Miriam, Luciana de Souza Leão, Arianna Legovini, Rogelio Marchetti, and Bilal Zia. “The impact of high school financial education: Evidence from a large-scale evaluation in Brazil.” American Economic Journal: Applied Economics 8, no. 4 (2016).</p>
<p>Carvalho, Leandro S., Stephan Meier, and Stephanie W. Wang. “Poverty and economic decision-making: Evidence from changes in financial resources at payday.” American Economic Review 106, no. 2 (2016): 260-84.</p>
<p>Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal, 2018.</p>
<p>Efron, Bradley. “Prediction, estimation, and attribution.” International Statistical Review 88 (2020): S28-S59. (<a href="https://efron.ckirby.su.domains/papers/2019PredictEstimatAttribut.pdf">link</a>)</p>
<p>Farbmacher, Helmut, Heinrich Kögel, and Martin Spindler. “Heterogeneous effects of poverty on attention.” Labour Economics 71 (2021): 102028.</p>
<p>Imbens, Guido W., and Donald B. Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.</p>
<p>Robins, James M., Andrea Rotnitzky, and Lue Ping Zhao. “Estimation of regression coefficients when some regressors are not always observed.” Journal of the American Statistical Association 89, no. 427 (1994): 846-866.</p>
<p>Stone, Charles J. “Optimal rates of convergence for nonparametric estimators.” The Annals of Statistics (1980): 1348-1360.</p>
<p>Nie, Xinkun, and Stefan Wager. “Quasi-oracle estimation of heterogeneous treatment effects.” Biometrika 108, no. 2 (2021): 299-319.</p>
<p>Robinson, Peter M. “Root-N-consistent semiparametric regression.” Econometrica: Journal of the Econometric Society (1988): 931-954.</p>
<p>Zheng, Wenjing, and Mark J. van der Laan. “Cross-validated targeted minimum-loss-based estimation.” In Targeted Learning, pp. 459-474. Springer, New York, NY, 2011.</p>
<p>Wager, Stefan, and Susan Athey. “Estimation and inference of heterogeneous treatment effects using random forests.” Journal of the American Statistical Association 113.523 (2018): 1228-1242.</p>
<p>Yadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. “Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.” arXiv preprint arXiv:2111.07966 (2021).</p>
<p>Zeileis, Achim, Torsten Hothorn, and Kurt Hornik. “Model-based recursive partitioning.” Journal of Computational and Graphical Statistics 17, no. 2 (2008): 492-514.</p>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p><code>grf</code> does not actually re-estimate a linear regression for each possible split point, that would be computationally infeasible. Instead, we estimate <span class="math inline">\(\hat \tau\)</span> <em>once</em> in the parent node, then use an approximation based on so-called “influence functions” to approximate how <span class="math inline">\(\hat \tau\)</span> would change if we moved a sample <span class="math inline">\(i\)</span> from the left to the right child.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>For a broader discussion of how Robinson’s transformation can be used to motivate a generic loss function for heterogeneous treatment effects estimation (the <em>R-learner</em>), as well as details on the role of <span class="math inline">\(e(x)\)</span> and <span class="math inline">\(m(x)\)</span>, see Nie &amp; Wager (2021).<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Note that <span class="math inline">\(\theta\)</span> does not have to be a scalar, the same construction could be used to target heterogeneity in a vector-valued target parameter by defining the criterion as the squared L2 norm: <span class="math inline">\(n_L n_R || \hat \tau_L - \hat \tau_R||^2\)</span>. This is the approach <code>grf</code>’s multi-arm causal forest takes.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Technically, the individual predictions <span class="math inline">\(\hat \tau(x)\)</span> have errors that are too big, and these errors don’t cancel out in the summary.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The <a href="https://grf-labs.github.io/policytree/reference/policy_tree.html">documentation</a> explains how this approach can be expected to scale to larger data sets (for example, reducing the <span class="math inline">\(X_j\)</span> cardinality leads to fewer split points to search over, and helps reduce the computational requirements of this approach).<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Julie Tibshirani, Susan Athey, Erik Sverdrup, Stefan Wager.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
